package com.ggp.utils.recall;

import com.ggp.*;

import java.util.HashMap;
import java.util.HashSet;
import java.util.List;

public class PerfectRecallExploitability {

    private static class ISActionValue {
        public double directValue = 0;
        public HashSet<IInformationSet> referencedInfoSets = null;

        public void add(IInformationSet is) {
            if (referencedInfoSets == null) {
                referencedInfoSets = new HashSet<>();
            }
            referencedInfoSets.add(is);
        }

        public void add(HashSet<IInformationSet> infoSets) {
            if (infoSets == null) return;
            if (referencedInfoSets == null) {
                referencedInfoSets = infoSets;
            } else {
                for (IInformationSet is: infoSets) {
                    referencedInfoSets.add(is);
                }
            }
        }

        public void add(double terminalValue) {
            directValue += terminalValue;
        }

        public double getFinalValue(HashMap<IInformationSet, ISInfo> bestResponseActionUtils) {
            double res = computeBestResponseUtility(bestResponseActionUtils, directValue, referencedInfoSets);
            referencedInfoSets = null;
            directValue = res;
            return directValue;
        }
    }

    private static class ISInfo {
        public ISActionValue[] actionValues;

        public ISInfo(int legalActionsSize) {
            actionValues = new ISActionValue[legalActionsSize];
            for (int i = 0; i < legalActionsSize; ++i) {
                actionValues[i] = new ISActionValue();
            }
        }

        public double getBestValue(HashMap<IInformationSet, ISInfo> bestResponseActionUtils) {
            double max = actionValues[0].getFinalValue(bestResponseActionUtils);
            for (int i = 1; i < actionValues.length; ++i) {
                double val = actionValues[i].getFinalValue(bestResponseActionUtils);
                if (val > max) max = val;
            }
            return max; // no need to cache the result as in perfect recall game it will be called exactly once
        }
    }

    private static class BestResponseUtility {
        /**
         * Counterfactual utility of terminal nodes for P1/P2.
         */
        public double p1Utility, p2Utility;

        /**
         * Information sets for P1/P2 the next time they play.
         */
        public HashSet<IInformationSet> p1ActIs, p2ActIs;

        public BestResponseUtility(double p1Utility, double p2Utility) {
            this.p1Utility = p1Utility;
            this.p2Utility = p2Utility;
        }

        public void add(BestResponseUtility util, int actingPlayer) {
            if (actingPlayer != 1) {
                this.p1Utility += util.p1Utility;
                this.p1ActIs = join(this.p1ActIs, util.p1ActIs);
            }
            if (actingPlayer != 2) {
                this.p2Utility += util.p2Utility;
                this.p2ActIs = join(this.p2ActIs, util.p2ActIs);
            }
        }

        private HashSet<IInformationSet> join(HashSet<IInformationSet> to, HashSet<IInformationSet> from) {
            if (from != null) {
                if (to == null) return from;
                else {
                    for (IInformationSet is: from) {
                        to.add(is);
                    }
                }
            }
            return to;
        }
    }

    private static BestResponseUtility fillBestResponseActionUtils(HashMap<IInformationSet, ISInfo> bestResponseActionUtils,
                                                                   IStrategy normalizedPlayerStrategy, ICompleteInformationState state,
                                                                   double p1ReachProb, double p2ReachProb) {
        if (p1ReachProb == 0 && p2ReachProb == 0) return new BestResponseUtility(0,0);
        if (state.isTerminal()) return new BestResponseUtility(state.getPayoff(1) * p2ReachProb,
                state.getPayoff(2) * p1ReachProb);

        List<IAction> legalActions = state.getLegalActions();
        BestResponseUtility ret = new BestResponseUtility(0,0);
        if (state.isRandomNode()) {
            double actionProb = 1d/legalActions.size();
            for (IAction a: legalActions) {
                BestResponseUtility tmp = fillBestResponseActionUtils(bestResponseActionUtils, normalizedPlayerStrategy,
                        state.next(a), p1ReachProb*actionProb, p2ReachProb*actionProb);
                ret.add(tmp, 0);
            }
            return ret;
        }
        IInformationSet is = state.getInfoSetForActingPlayer();
        final int actPlayer = state.getActingPlayerId();

        ISInfo info = bestResponseActionUtils.computeIfAbsent(is, k -> new ISInfo(legalActions.size()));
        int actionIdx = 0;
        for (IAction a: legalActions) {
            double actionProb = normalizedPlayerStrategy.getProbability(is, a);
            double newP1Prob = p1ReachProb, newP2Prob = p2ReachProb;
            if (actPlayer == 1) {
                newP1Prob *= actionProb;
            } else {
                newP2Prob *= actionProb;
            }
            BestResponseUtility tmp = fillBestResponseActionUtils(bestResponseActionUtils, normalizedPlayerStrategy, state.next(a), newP1Prob, newP2Prob);
            ret.add(tmp, actPlayer);
            HashSet<IInformationSet> nextActInfoSets;
            double directValue;
            if (actPlayer == 1) {
                nextActInfoSets = tmp.p1ActIs;
                directValue = tmp.p1Utility;
            } else {
                nextActInfoSets = tmp.p2ActIs;
                directValue = tmp.p2Utility;
            }
            info.actionValues[actionIdx].add(directValue);
            info.actionValues[actionIdx].add(nextActInfoSets);
            actionIdx++;
        }
        if (actPlayer == 1) {
            ret.p1ActIs = new HashSet<>();
            ret.p1ActIs.add(is);
            ret.p1Utility = 0;
        } else {
            ret.p2ActIs = new HashSet<>();
            ret.p2ActIs.add(is);
            ret.p2Utility = 0;
        }
        return ret;
    }

    /**
     * Get utility of best response strategy for node with given direct value and referenced info sets
     * @param bestResponseActionUtils
     * @param directValue
     * @param referencedInfoSets
     * @return
     */
    private static double computeBestResponseUtility(HashMap<IInformationSet, ISInfo> bestResponseActionUtils, double directValue, HashSet<IInformationSet> referencedInfoSets) {
        if (referencedInfoSets != null) {
            for (IInformationSet is: referencedInfoSets) {
                directValue += bestResponseActionUtils.get(is).getBestValue(bestResponseActionUtils);
            }
        }
        return directValue;
    }

    public static double computeExploitability(IStrategy normalizedStratForBothPlayers, IGameDescription gameDescription) {
        return computeExploitability(normalizedStratForBothPlayers, gameDescription.getInitialState());
    }

    public static double computeExploitability(IStrategy normalizedStratForBothPlayers, ICompleteInformationState initialState) {
        HashMap<IInformationSet, ISInfo> bestResponseActionUtils = new HashMap<>();
        BestResponseUtility utilities = fillBestResponseActionUtils(bestResponseActionUtils, normalizedStratForBothPlayers, initialState, 1, 1);

        return 0.5d * (computeBestResponseUtility(bestResponseActionUtils, utilities.p1Utility, utilities.p1ActIs)
                + computeBestResponseUtility(bestResponseActionUtils, utilities.p2Utility, utilities.p2ActIs));
    }
}
